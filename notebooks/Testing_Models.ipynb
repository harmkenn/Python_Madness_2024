{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A\n",
      "0  aa\n",
      "1  bb\n",
      "2  cc\n",
      "3  dd\n",
      "4  ee\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'A': ['aa ', 'bb  12', 'cc 3', 'dd  456', 'ee 789  ']})\n",
    "\n",
    "def remove_trailing_space_or_digit(text):\n",
    "  \"\"\"\n",
    "  Removes trailing spaces or digits from a string.\n",
    "\n",
    "  Args:\n",
    "    text: The string to modify.\n",
    "\n",
    "  Returns:\n",
    "    The modified string with trailing spaces or digits removed.\n",
    "  \"\"\"\n",
    "  while text and (text[-1].isspace() or text[-1].isdigit()):\n",
    "    text = text[:-1]\n",
    "  return text\n",
    "\n",
    "# Apply the lambda function to the 'A' column\n",
    "df['A'] = df['A'].apply(remove_trailing_space_or_digit)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are several approaches you can consider for modeling your data in Python with 40 quantitative inputs and 2 quantitative outputs:\n",
    "\n",
    "1. Supervised Learning with Regularization:\n",
    "\n",
    "Linear Regression: A classic approach for modeling linear relationships between features and outputs. Use regularization techniques like Ridge or Lasso to handle large numbers of features and potential collinearity.\n",
    "Support Vector Regression (SVR): Effective for non-linear relationships and robust to outliers.\n",
    "Decision Tree Regression: Can capture complex relationships without feature engineering. Consider Random Forest Regression for improved stability.\n",
    "2. Neural Networks:\n",
    "\n",
    "Multi-Layer Perceptron (MLP): A deep learning model adaptable to diverse relationships. Fine-tune hyperparameters like the number of layers and neurons.\n",
    "Long Short-Term Memory (LSTM) Networks: Effective for analyzing sequential data if your inputs have inherent order or time dependence.\n",
    "3. Feature Engineering and Selection:\n",
    "\n",
    "Dimensionality Reduction: Use techniques like Principal Component Analysis (PCA) to reduce the number of features, especially if computational resources are limited.\n",
    "Feature Selection: Identify the most relevant features using techniques like correlation analysis or information gain.\n",
    "4. Evaluation and Improvement:\n",
    "\n",
    "Metrics: Choose appropriate metrics like Mean Squared Error (MSE) or R-squared to evaluate model performance.\n",
    "Cross-validation: Split your data and validate your model's generalizability.\n",
    "Hyperparameter Tuning: Experiment with different hyperparameters for each model to improve performance.\n",
    "Additional points:\n",
    "\n",
    "Data Normalization or Scaling: Normalize or scale your input features to ensure they have similar scales, especially if using distance-based algorithms like SVR.\n",
    "Missing Data Handling: Check for missing values and consider imputation techniques.\n",
    "Software Recommendations: Popular libraries include scikit-learn, TensorFlow, PyTorch, and Keras. Choose the one that suits your experience and project requirements.\n",
    "\n",
    "I have 4 columns of quantitative inputs and 2 columns of quantitative outputs in a pandas dataframe. I need a way to model the data in python.\n",
    "Can you give me some code (with sample data) for Long Short-Term Memory (LSTM) Networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770 219\n"
     ]
    }
   ],
   "source": [
    "#Polynomial regression with PCA\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Sample data with 4 quantitative inputs and 2 quantitative outputs\n",
    "data = pd.read_csv('step07_FUStats.csv')\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = df.dropna()\n",
    "X = df[['AFSeed', 'AUSeed', 'AdjEM_x', \n",
    "       'AdjO_x', 'AdjD_x', 'AdjT_x', 'SOSEM_x', 'SOSO_x', 'SOSD_x', 'BPI(O)_x',\n",
    "       'BPI(D)_x', 'W_x', 'L_x', 'Pts_x', 'Opp_x', 'MOV_x', 'SOS_x', 'OSRS_x',\n",
    "       'DSRS_x', 'SRS_x', 'PASE_x', 'AdjEM_y', 'AdjO_y', 'AdjD_y',\n",
    "       'AdjT_y', 'SOSEM_y', 'SOSO_y', 'SOSD_y', 'BPI(O)_y', 'BPI(D)_y', 'W_y',\n",
    "       'L_y', 'Pts_y', 'Opp_y', 'MOV_y', 'SOS_y', 'OSRS_y', 'DSRS_y', 'SRS_y', 'PASE_y']]\n",
    "y = df[['AFScore', 'AUScore']]\n",
    "\n",
    "# Use PCA to reduce the dimensionality of the dataset\n",
    "pca = PCA(n_components=14)\n",
    "inputs = pca.fit_transform(X)\n",
    "\n",
    "# Use polynomial modeling to fit the data\n",
    "poly = PolynomialFeatures(degree=4)\n",
    "inputs_poly = poly.fit_transform(inputs)\n",
    "model = LinearRegression().fit(inputs_poly, y)\n",
    "\n",
    "# Generate new data and corresponding outputs\n",
    "\n",
    "new_inputs = pca.transform(X)\n",
    "new_inputs_poly = poly.transform(new_inputs)\n",
    "new_outputs = model.predict(new_inputs_poly)\n",
    "\n",
    "# Print the new data and corresponding outputs\n",
    "\n",
    "check = pd.concat([X,y, pd.DataFrame(new_outputs, columns=['output1', 'output2'])], axis=1)\n",
    "check['real'] = check['AFScore'] - check['AUScore']\n",
    "check['predicted'] = check['output1'] - check['output2']\n",
    "check = check.dropna()\n",
    "\n",
    "correct = (check['real'] * check['predicted'] > 0).sum()\n",
    "incorrect = (check['real'] * check['predicted'] < 0).sum()\n",
    "\n",
    "print(correct,incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701 288\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import MultiTaskElasticNet\n",
    "\n",
    "# Sample data with 4 quantitative inputs and 2 quantitative outputs\n",
    "data = pd.read_csv('step07_FUStats.csv')\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = df.dropna()\n",
    "X = df[['AFSeed', 'AUSeed', 'AdjEM_x', \n",
    "       'AdjO_x', 'AdjD_x', 'AdjT_x', 'SOSEM_x', 'SOSO_x', 'SOSD_x', 'BPI(O)_x',\n",
    "       'BPI(D)_x', 'W_x', 'L_x', 'Pts_x', 'Opp_x', 'MOV_x', 'SOS_x', 'OSRS_x',\n",
    "       'DSRS_x', 'SRS_x', 'PASE_x', 'AdjEM_y', 'AdjO_y', 'AdjD_y',\n",
    "       'AdjT_y', 'SOSEM_y', 'SOSO_y', 'SOSD_y', 'BPI(O)_y', 'BPI(D)_y', 'W_y',\n",
    "       'L_y', 'Pts_y', 'Opp_y', 'MOV_y', 'SOS_y', 'OSRS_y', 'DSRS_y', 'SRS_y', 'PASE_y']]\n",
    "y = df[['AFScore', 'AUScore']]\n",
    "\n",
    "# Create a MultiTaskElasticNet model with regularization\n",
    "model = MultiTaskElasticNet(alpha=0.1)  # Adjust alpha for regularization strength\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Print the new data and corresponding outputs\n",
    "\n",
    "check = pd.concat([X,y, pd.DataFrame(y_pred, columns=['output1', 'output2'])], axis=1)\n",
    "check['real'] = check['AFScore'] - check['AUScore']\n",
    "check['predicted'] = check['output1'] - check['output2']\n",
    "check = check.dropna()\n",
    "\n",
    "correct = (check['real'] * check['predicted'] > 0).sum()\n",
    "incorrect = (check['real'] * check['predicted'] < 0).sum()\n",
    "\n",
    "print(correct,incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712 277\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Regression\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Sample data with 4 quantitative inputs and 2 quantitative outputs\n",
    "data = pd.read_csv('step07_FUStats.csv')\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = df.dropna()\n",
    "X = df[['AFSeed', 'AUSeed', 'AdjEM_x', \n",
    "       'AdjO_x', 'AdjD_x', 'AdjT_x', 'SOSEM_x', 'SOSO_x', 'SOSD_x', 'BPI(O)_x',\n",
    "       'BPI(D)_x', 'W_x', 'L_x', 'Pts_x', 'Opp_x', 'MOV_x', 'SOS_x', 'OSRS_x',\n",
    "       'DSRS_x', 'SRS_x', 'PASE_x', 'AdjEM_y', 'AdjO_y', 'AdjD_y',\n",
    "       'AdjT_y', 'SOSEM_y', 'SOSO_y', 'SOSD_y', 'BPI(O)_y', 'BPI(D)_y', 'W_y',\n",
    "       'L_y', 'Pts_y', 'Opp_y', 'MOV_y', 'SOS_y', 'OSRS_y', 'DSRS_y', 'SRS_y', 'PASE_y']]\n",
    "winner = df[['AFScore']]\n",
    "looser = df[['AUScore']]\n",
    "\n",
    "# Create SVR models for each output\n",
    "model1 = SVR()\n",
    "model2 = SVR()\n",
    "\n",
    "# Fit models to the training data\n",
    "model1.fit(X, winner)\n",
    "model2.fit(X, looser)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y1_pred = model1.predict(X)\n",
    "y2_pred = model2.predict(X)\n",
    "\n",
    "# Print the new data and corresponding outputs\n",
    "\n",
    "check = pd.concat([X,winner,looser, pd.DataFrame({'output1':y1_pred,'output2':y2_pred})], axis=1)\n",
    "check['real'] = check['AFScore'] - check['AUScore']\n",
    "check['predicted'] = check['output1'] - check['output2']\n",
    "check = check.dropna()\n",
    "\n",
    "correct = (check['real'] * check['predicted'] > 0).sum()\n",
    "incorrect = (check['real'] * check['predicted'] < 0).sum()\n",
    "\n",
    "print(correct,incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770 218\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Regression\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "# Sample data with 4 quantitative inputs and 2 quantitative outputs\n",
    "data = pd.read_csv('step07_FUStats.csv')\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = df.dropna()\n",
    "X = df[['AFSeed', 'AUSeed', 'AdjEM_x', \n",
    "       'AdjO_x', 'AdjD_x', 'AdjT_x', 'SOSEM_x', 'SOSO_x', 'SOSD_x', 'BPI(O)_x',\n",
    "       'BPI(D)_x', 'W_x', 'L_x', 'Pts_x', 'Opp_x', 'MOV_x', 'SOS_x', 'OSRS_x',\n",
    "       'DSRS_x', 'SRS_x', 'PASE_x', 'AdjEM_y', 'AdjO_y', 'AdjD_y',\n",
    "       'AdjT_y', 'SOSEM_y', 'SOSO_y', 'SOSD_y', 'BPI(O)_y', 'BPI(D)_y', 'W_y',\n",
    "       'L_y', 'Pts_y', 'Opp_y', 'MOV_y', 'SOS_y', 'OSRS_y', 'DSRS_y', 'SRS_y', 'PASE_y']]\n",
    "y = df[['AFScore', 'AUScore']]\n",
    "\n",
    "\n",
    "# Create and tune the model\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Print the new data and corresponding outputs\n",
    "\n",
    "check = pd.concat([X,y, pd.DataFrame(y_pred, columns=['output1', 'output2'])], axis=1)\n",
    "check['real'] = check['AFScore'] - check['AUScore']\n",
    "check['predicted'] = check['output1'] - check['output2']\n",
    "check = check.dropna()\n",
    "\n",
    "correct = (check['real'] * check['predicted'] > 0).sum()\n",
    "incorrect = (check['real'] * check['predicted'] < 0).sum()\n",
    "\n",
    "print(correct,incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676 313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# MultiLayer Perception\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Sample data with 4 quantitative inputs and 2 quantitative outputs\n",
    "data = pd.read_csv('step07_FUStats.csv')\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = df.dropna()\n",
    "X = df[['AFSeed', 'AUSeed', 'AdjEM_x', \n",
    "       'AdjO_x', 'AdjD_x', 'AdjT_x', 'SOSEM_x', 'SOSO_x', 'SOSD_x', 'BPI(O)_x',\n",
    "       'BPI(D)_x', 'W_x', 'L_x', 'Pts_x', 'Opp_x', 'MOV_x', 'SOS_x', 'OSRS_x',\n",
    "       'DSRS_x', 'SRS_x', 'PASE_x', 'AdjEM_y', 'AdjO_y', 'AdjD_y',\n",
    "       'AdjT_y', 'SOSEM_y', 'SOSO_y', 'SOSD_y', 'BPI(O)_y', 'BPI(D)_y', 'W_y',\n",
    "       'L_y', 'Pts_y', 'Opp_y', 'MOV_y', 'SOS_y', 'OSRS_y', 'DSRS_y', 'SRS_y', 'PASE_y']]\n",
    "y = df[['AFScore', 'AUScore']]\n",
    "\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=(10, 5),  # Two hidden layers with 10 and 5 neurons\n",
    "    activation='relu',  # ReLU activation function\n",
    "    solver='adam',  # Adam optimizer\n",
    "    max_iter=500  # Maximum number of iterations\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "mlp.fit(X, y)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = mlp.predict(X)\n",
    "\n",
    "# Print the new data and corresponding outputs\n",
    "\n",
    "check = pd.concat([X,y, pd.DataFrame(y_pred, columns=['output1', 'output2'])], axis=1)\n",
    "check['real'] = check['AFScore'] - check['AUScore']\n",
    "check['predicted'] = check['output1'] - check['output2']\n",
    "check = check.dropna()\n",
    "\n",
    "correct = (check['real'] * check['predicted'] > 0).sum()\n",
    "incorrect = (check['real'] * check['predicted'] < 0).sum()\n",
    "\n",
    "print(correct,incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Deep Neural Networks (DNNs):\n",
    "\n",
    "Pros: Powerful and flexible, can capture complex non-linear relationships, suitable for large datasets.\n",
    "Cons: Can be computationally expensive, might require data pre-processing and hyperparameter tuning, black-box nature makes interpretation challenging.\n",
    "\n",
    "2. Gradient Boosting Machines (GBMs):\n",
    "\n",
    "Pros: Handles complex interactions between features, robust to outliers, good interpretability through feature importance scores.\n",
    "Cons: May not perform as well as DNNs on very complex datasets, hyperparameter tuning is important.\n",
    "\n",
    "3. Transformer-based models:\n",
    "\n",
    "Pros: State-of-the-art for sequential data and natural language processing, potentially good for capturing long-range dependencies between features.\n",
    "Cons: Relatively new technology, computationally expensive, requires large datasets to train effectively.\n",
    "\n",
    "4. AutoML tools:\n",
    "\n",
    "Pros: Automate model selection and hyperparameter tuning, convenient for non-experts.\n",
    "Cons: Limited interpretability, black-box nature makes understanding the model less transparent.\n",
    "\n",
    "5. Multi-Output Learning Techniques:\n",
    "\n",
    "Pros: Specifically designed for handling multiple outputs, various approaches available like multi-task learning, ensemble methods with shared representations.\n",
    "Cons: Complexity depends on the chosen technique, understanding interactions between outputs might be challenging.\n",
    "\n",
    "I have 4 columns of quantitative inputs and 2 columns of quantitative outputs in a pandas dataframe. I need a way to model the data in python.\n",
    "Can you give me some code (with sample data) for Long Short-Term Memory (LSTM) Networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770 218\n"
     ]
    }
   ],
   "source": [
    "# Transformer Based Models\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "# Sample data with 4 quantitative inputs and 2 quantitative outputs\n",
    "data = pd.read_csv('step07_FUStats.csv')\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = df.dropna()\n",
    "X = df[['AFSeed', 'AUSeed', 'AdjEM_x', \n",
    "       'AdjO_x', 'AdjD_x', 'AdjT_x', 'SOSEM_x', 'SOSO_x', 'SOSD_x', 'BPI(O)_x',\n",
    "       'BPI(D)_x', 'W_x', 'L_x', 'Pts_x', 'Opp_x', 'MOV_x', 'SOS_x', 'OSRS_x',\n",
    "       'DSRS_x', 'SRS_x', 'PASE_x', 'AdjEM_y', 'AdjO_y', 'AdjD_y',\n",
    "       'AdjT_y', 'SOSEM_y', 'SOSO_y', 'SOSD_y', 'BPI(O)_y', 'BPI(D)_y', 'W_y',\n",
    "       'L_y', 'Pts_y', 'Opp_y', 'MOV_y', 'SOS_y', 'OSRS_y', 'DSRS_y', 'SRS_y', 'PASE_y']]\n",
    "y = df[['AFScore', 'AUScore']]\n",
    "\n",
    "# Define transformers\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "\n",
    "# Specify which columns are numeric and categorical\n",
    "numeric_features = ['AFSeed', 'AUSeed', 'AdjEM_x', \n",
    "       'AdjO_x', 'AdjD_x', 'AdjT_x', 'SOSEM_x', 'SOSO_x', 'SOSD_x', 'BPI(O)_x',\n",
    "       'BPI(D)_x', 'W_x', 'L_x', 'Pts_x', 'Opp_x', 'MOV_x', 'SOS_x', 'OSRS_x',\n",
    "       'DSRS_x', 'SRS_x', 'PASE_x', 'AdjEM_y', 'AdjO_y', 'AdjD_y',\n",
    "       'AdjT_y', 'SOSEM_y', 'SOSO_y', 'SOSD_y', 'BPI(O)_y', 'BPI(D)_y', 'W_y',\n",
    "       'L_y', 'Pts_y', 'Opp_y', 'MOV_y', 'SOS_y', 'OSRS_y', 'DSRS_y', 'SRS_y', 'PASE_y']\n",
    "\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "    ])\n",
    "\n",
    "# Fit and transform the data\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Convert the transformed data back to a pandas dataframe\n",
    "X = pd.DataFrame(X_transformed, columns=numeric_features)\n",
    "\n",
    "# Create and tune the model\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Print the new data and corresponding outputs\n",
    "\n",
    "check = pd.concat([X,y, pd.DataFrame(y_pred, columns=['output1', 'output2'])], axis=1)\n",
    "check['real'] = check['AFScore'] - check['AUScore']\n",
    "check['predicted'] = check['output1'] - check['output2']\n",
    "check = check.dropna()\n",
    "\n",
    "correct = (check['real'] * check['predicted'] > 0).sum()\n",
    "incorrect = (check['real'] * check['predicted'] < 0).sum()\n",
    "\n",
    "print(correct,incorrect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770 218\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "# Sample data with 4 quantitative inputs and 2 quantitative outputs\n",
    "data = pd.read_csv('step07_FUStats.csv')\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = df.dropna()\n",
    "X = df[['AFSeed', 'AUSeed', 'AdjEM_x', \n",
    "       'AdjO_x', 'AdjD_x', 'AdjT_x', 'SOSEM_x', 'SOSO_x', 'SOSD_x', 'BPI(O)_x',\n",
    "       'BPI(D)_x', 'W_x', 'L_x', 'Pts_x', 'Opp_x', 'MOV_x', 'SOS_x', 'OSRS_x',\n",
    "       'DSRS_x', 'SRS_x', 'PASE_x', 'AdjEM_y', 'AdjO_y', 'AdjD_y',\n",
    "       'AdjT_y', 'SOSEM_y', 'SOSO_y', 'SOSD_y', 'BPI(O)_y', 'BPI(D)_y', 'W_y',\n",
    "       'L_y', 'Pts_y', 'Opp_y', 'MOV_y', 'SOS_y', 'OSRS_y', 'DSRS_y', 'SRS_y', 'PASE_y']]\n",
    "y1 = df['AFScore']\n",
    "y2 = df['AUScore']\n",
    "\n",
    "# Create and fit GBM models\n",
    "model1 = GradientBoostingRegressor(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
    "model1.fit(X, y1)\n",
    "\n",
    "model2 = GradientBoostingRegressor(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
    "model2.fit(X, y2)\n",
    "\n",
    "# Make predictions\n",
    "y_pred1 = model1.predict(X)\n",
    "y_pred2 = model2.predict(X)\n",
    "\n",
    "# Print the new data and corresponding outputs\n",
    "\n",
    "check = pd.concat([X,y1,y2, pd.DataFrame(y_pred, columns=['output1', 'output2'])], axis=1)\n",
    "check['real'] = check['AFScore'] - check['AUScore']\n",
    "check['predicted'] = check['output1'] - check['output2']\n",
    "check = check.dropna()\n",
    "\n",
    "correct = (check['real'] * check['predicted'] > 0).sum()\n",
    "incorrect = (check['real'] * check['predicted'] < 0).sum()\n",
    "\n",
    "print(correct,incorrect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701 288\n"
     ]
    }
   ],
   "source": [
    "# MultiTaskElasticNet\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import MultiTaskElasticNet\n",
    "\n",
    "\n",
    "\n",
    "# Sample data with 4 quantitative inputs and 2 quantitative outputs\n",
    "data = pd.read_csv('step07_FUStats.csv')\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = df.dropna()\n",
    "X = df[['AFSeed', 'AUSeed', 'AdjEM_x', \n",
    "       'AdjO_x', 'AdjD_x', 'AdjT_x', 'SOSEM_x', 'SOSO_x', 'SOSD_x', 'BPI(O)_x',\n",
    "       'BPI(D)_x', 'W_x', 'L_x', 'Pts_x', 'Opp_x', 'MOV_x', 'SOS_x', 'OSRS_x',\n",
    "       'DSRS_x', 'SRS_x', 'PASE_x', 'AdjEM_y', 'AdjO_y', 'AdjD_y',\n",
    "       'AdjT_y', 'SOSEM_y', 'SOSO_y', 'SOSD_y', 'BPI(O)_y', 'BPI(D)_y', 'W_y',\n",
    "       'L_y', 'Pts_y', 'Opp_y', 'MOV_y', 'SOS_y', 'OSRS_y', 'DSRS_y', 'SRS_y', 'PASE_y']]\n",
    "y = df[['AFScore', 'AUScore']]\n",
    "\n",
    "# Create and train MultiTaskElasticNet model\n",
    "model = MultiTaskElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Print the new data and corresponding outputs\n",
    "\n",
    "check = pd.concat([X,y, pd.DataFrame(y_pred, columns=['output1', 'output2'])], axis=1)\n",
    "check['real'] = check['AFScore'] - check['AUScore']\n",
    "check['predicted'] = check['output1'] - check['output2']\n",
    "check = check.dropna()\n",
    "\n",
    "correct = (check['real'] * check['predicted'] > 0).sum()\n",
    "incorrect = (check['real'] * check['predicted'] < 0).sum()\n",
    "\n",
    "print(correct,incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748 240\n"
     ]
    }
   ],
   "source": [
    "# RandonForestRegressor\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Sample data with 4 quantitative inputs and 2 quantitative outputs\n",
    "data = pd.read_csv('step07_FUStats.csv')\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = df.dropna()\n",
    "X = df[['AFSeed', 'AUSeed', 'AdjEM_x', \n",
    "       'AdjO_x', 'AdjD_x', 'AdjT_x', 'SOSEM_x', 'SOSO_x', 'SOSD_x', 'BPI(O)_x',\n",
    "       'BPI(D)_x', 'W_x', 'L_x', 'Pts_x', 'Opp_x', 'MOV_x', 'SOS_x', 'OSRS_x',\n",
    "       'DSRS_x', 'SRS_x', 'PASE_x', 'AdjEM_y', 'AdjO_y', 'AdjD_y',\n",
    "       'AdjT_y', 'SOSEM_y', 'SOSO_y', 'SOSD_y', 'BPI(O)_y', 'BPI(D)_y', 'W_y',\n",
    "       'L_y', 'Pts_y', 'Opp_y', 'MOV_y', 'SOS_y', 'OSRS_y', 'DSRS_y', 'SRS_y', 'PASE_y']]\n",
    "y = df[['AFScore', 'AUScore']]\n",
    "\n",
    "# Create and train Multi-Output Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Print the new data and corresponding outputs\n",
    "\n",
    "check = pd.concat([X,y, pd.DataFrame(y_pred, columns=['output1', 'output2'])], axis=1)\n",
    "check['real'] = check['AFScore'] - check['AUScore']\n",
    "check['predicted'] = check['output1'] - check['output2']\n",
    "check = check.dropna()\n",
    "\n",
    "correct = (check['real'] * check['predicted'] > 0).sum()\n",
    "incorrect = (check['real'] * check['predicted'] < 0).sum()\n",
    "\n",
    "print(correct,incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "707 282\n"
     ]
    }
   ],
   "source": [
    "# MultiOutputRegressor\n",
    "import pandas as pd\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "# Sample data with 4 quantitative inputs and 2 quantitative outputs\n",
    "data = pd.read_csv('step07_FUStats.csv')\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = df.dropna()\n",
    "X = df[['AFSeed', 'AUSeed', 'AdjEM_x', \n",
    "       'AdjO_x', 'AdjD_x', 'AdjT_x', 'SOSEM_x', 'SOSO_x', 'SOSD_x', 'BPI(O)_x',\n",
    "       'BPI(D)_x', 'W_x', 'L_x', 'Pts_x', 'Opp_x', 'MOV_x', 'SOS_x', 'OSRS_x',\n",
    "       'DSRS_x', 'SRS_x', 'PASE_x', 'AdjEM_y', 'AdjO_y', 'AdjD_y',\n",
    "       'AdjT_y', 'SOSEM_y', 'SOSO_y', 'SOSD_y', 'BPI(O)_y', 'BPI(D)_y', 'W_y',\n",
    "       'L_y', 'Pts_y', 'Opp_y', 'MOV_y', 'SOS_y', 'OSRS_y', 'DSRS_y', 'SRS_y', 'PASE_y']]\n",
    "y = df[['AFScore', 'AUScore']]\n",
    "\n",
    "# Define the model\n",
    "model = MultiOutputRegressor(LinearRegression())\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Print the new data and corresponding outputs\n",
    "\n",
    "check = pd.concat([X,y, pd.DataFrame(y_pred, columns=['output1', 'output2'])], axis=1)\n",
    "check['real'] = check['AFScore'] - check['AUScore']\n",
    "check['predicted'] = check['output1'] - check['output2']\n",
    "check = check.dropna()\n",
    "\n",
    "correct = (check['real'] * check['predicted'] > 0).sum()\n",
    "incorrect = (check['real'] * check['predicted'] < 0).sum()\n",
    "\n",
    "print(correct,incorrect)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75a0be042a4beffa7bfe652d9ca9055e675071b6b5ebd40357e1d67e4cb4d822"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
