{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the initial year\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "y = 2008\n",
    "keep = ['Year','Team','W', 'L', 'Pts', 'Opp', 'MOV','SOS', 'OSRS', 'DSRS', 'SRS']\n",
    "\n",
    "# Fetch the HTML content\n",
    "url = f\"https://www.sports-reference.com/cbb/seasons/{y}-ratings.html\"  # Replace with the actual URL\n",
    "data = requests.get(url)\n",
    "\n",
    "# Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(data.content, \"html.parser\")\n",
    "\n",
    "# Find the table element\n",
    "table = soup.find(\"table\")  # Adjust the selector if needed (e.g., by class or id)\n",
    "\n",
    "# Extract table data\n",
    "datatable = []\n",
    "for row in table.find_all(\"tr\"):\n",
    "    row_data = []\n",
    "    for cell in row.find_all([\"td\", \"th\"]):\n",
    "        # Extract text content, handling potential formatting tags\n",
    "        cell_text = cell.get_text(strip=True)\n",
    "        row_data.append(cell_text)\n",
    "    datatable.append(row_data)\n",
    "\n",
    "# Print the extracted table data\n",
    "\n",
    "datatable = pd.DataFrame(datatable)\n",
    "datatable.columns = datatable.iloc[1]\n",
    "datatable = datatable.dropna(subset=['W'])\n",
    "datatable = datatable[datatable['W']!='W']\n",
    "datatable['Team'] = datatable['School']\n",
    "datatable['Year'] = y\n",
    "\n",
    "datatable = datatable[keep]\n",
    "\n",
    "datatable.to_csv('step03_br0824.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Basketball Reference 2008 to 2024\n",
    "# https://www.sports-reference.com/cbb/seasons/2008-ratings.html\n",
    "# Importing the required libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "keep = ['Year','Team','W', 'L', 'Pts', 'Opp', 'MOV','SOS', 'OSRS', 'DSRS', 'SRS']\n",
    "\n",
    "allBR = pd.DataFrame(columns = keep)\n",
    "\n",
    "for y in range(2008,2009):\n",
    "    # Fetch the HTML content\n",
    "    url = f\"https://www.sports-reference.com/cbb/seasons/{y}-ratings.html\"  # Replace with the actual URL\n",
    "    data = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(data.content, \"html.parser\")\n",
    "\n",
    "    # Find the table element\n",
    "    table = soup.find(\"table\")  # Adjust the selector if needed (e.g., by class or id)\n",
    "\n",
    "    # Extract table data\n",
    "    datatable = []\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        row_data = []\n",
    "        for cell in row.find_all([\"td\", \"th\"]):\n",
    "            # Extract text content, handling potential formatting tags\n",
    "            cell_text = cell.get_text(strip=True)\n",
    "            row_data.append(cell_text)\n",
    "        datatable.append(row_data)\n",
    "\n",
    "    # Print the extracted table data\n",
    "\n",
    "    datatable = pd.DataFrame(datatable)\n",
    "    datatable.columns = datatable.iloc[1]\n",
    "    datatable = datatable.dropna(subset=['W'])\n",
    "    datatable = datatable[datatable['W']!='W']\n",
    "    datatable['Team'] = datatable['School']\n",
    "    datatable['Year'] = y\n",
    "\n",
    "    datatable = datatable[keep]\n",
    "\n",
    "    allBR = pd.concat([allBR,datatable])\n",
    "allBR.to_csv('step03_br0824.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an additional season\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "y = 2024\n",
    "keep = ['Year','Team','W', 'L', 'Pts', 'Opp', 'MOV','SOS', 'OSRS', 'DSRS', 'SRS']\n",
    "\n",
    "allBR = pd.read_csv('step03_br0824.csv')\n",
    "allBR =allBR[allBR['Year']<y]\n",
    "\n",
    "# Fetch the HTML content\n",
    "url = f\"https://www.sports-reference.com/cbb/seasons/{y}-ratings.html\"  # Replace with the actual URL\n",
    "data = requests.get(url)\n",
    "\n",
    "# Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(data.content, \"html.parser\")\n",
    "\n",
    "# Find the table element\n",
    "table = soup.find(\"table\")  # Adjust the selector if needed (e.g., by class or id)\n",
    "\n",
    "# Extract table data\n",
    "datatable = []\n",
    "for row in table.find_all(\"tr\"):\n",
    "    row_data = []\n",
    "    for cell in row.find_all([\"td\", \"th\"]):\n",
    "        # Extract text content, handling potential formatting tags\n",
    "        cell_text = cell.get_text(strip=True)\n",
    "        row_data.append(cell_text)\n",
    "    datatable.append(row_data)\n",
    "\n",
    "# Print the extracted table data\n",
    "\n",
    "datatable = pd.DataFrame(datatable)\n",
    "datatable.columns = datatable.iloc[1]\n",
    "datatable = datatable.dropna(subset=['W'])\n",
    "datatable = datatable[datatable['W']!='W']\n",
    "datatable['Team'] = datatable['School']\n",
    "datatable['Year'] = y\n",
    "\n",
    "datatable = datatable[keep]\n",
    "\n",
    "allBR = pd.concat([allBR,datatable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Read HTML tables using read_html\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m tables \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Select the desired table (adjust index if needed)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m df \u001b[38;5;241m=\u001b[39m tables[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Assuming the first table is the one you want\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\html.py:1246\u001b[0m, in \u001b[0;36mread_html\u001b[1;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend, storage_options)\u001b[0m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m   1231\u001b[0m     [\n\u001b[0;32m   1232\u001b[0m         is_file_like(io),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1236\u001b[0m     ]\n\u001b[0;32m   1237\u001b[0m ):\n\u001b[0;32m   1238\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal html to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_html\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1240\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1243\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1244\u001b[0m     )\n\u001b[1;32m-> 1246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\html.py:1009\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1008\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m retained \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[1;32m-> 1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retained\n\u001b[0;32m   1011\u001b[0m ret \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\html.py:989\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    978\u001b[0m p \u001b[38;5;241m=\u001b[39m parser(\n\u001b[0;32m    979\u001b[0m     io,\n\u001b[0;32m    980\u001b[0m     compiled_match,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    985\u001b[0m     storage_options,\n\u001b[0;32m    986\u001b[0m )\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 989\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m caught:\n\u001b[0;32m    991\u001b[0m     \u001b[38;5;66;03m# if `io` is an io-like object, check if it's seekable\u001b[39;00m\n\u001b[0;32m    992\u001b[0m     \u001b[38;5;66;03m# and try to rewind it before trying the next parser\u001b[39;00m\n\u001b[0;32m    993\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(io, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseekable\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m io\u001b[38;5;241m.\u001b[39mseekable():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\html.py:249\u001b[0m, in \u001b[0;36m_HtmlFrameParser.parse_tables\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_tables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    242\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m    Parse and return all tables from the DOM.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03m    list of parsed (header, body, footer) tuples from tables.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 249\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_tables(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs)\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_thead_tbody_tfoot(table) \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\html.py:667\u001b[0m, in \u001b[0;36m_BeautifulSoupHtml5LibFrameParser._build_doc\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    664\u001b[0m     udoc \u001b[38;5;241m=\u001b[39m bdoc\n\u001b[0;32m    665\u001b[0m     from_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding\n\u001b[1;32m--> 667\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mudoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhtml5lib\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_encoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m br \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbr\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    670\u001b[0m     br\u001b[38;5;241m.\u001b[39mreplace_with(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m br\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\bs4\\__init__.py:250\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m     builder_class \u001b[38;5;241m=\u001b[39m builder_registry\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;241m*\u001b[39mfeatures)\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[0;32m    251\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a tree builder with the features you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Do you need to install a parser library?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(features))\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Specify the URL\n",
    "url = \"https://www.sports-reference.com/cbb/seasons/men/2008-ratings.html\"  # Replace with the actual URL\n",
    "\n",
    "# Fetch the HTML content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Read HTML tables using read_html\n",
    "tables = pd.read_html(response.content)\n",
    "\n",
    "# Select the desired table (adjust index if needed)\n",
    "df = tables[0]  # Assuming the first table is the one you want\n",
    "\n",
    "# Print the scraped DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the data to a CSV file (optional)\n",
    "# df.to_csv(\"scraped_table.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Creighton (15)</td>\n",
       "      <td>66</td>\n",
       "      <td>Final</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>St. John's (NY)</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0   1      2\n",
       "0   Creighton (15)  66  Final\n",
       "1  St. John's (NY)  80    NaN"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.sports-reference.com/cbb/seasons/men/2008-ratings.html\n",
    "\n",
    "# Just 2008 to start\n",
    "\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import io\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "service = Service(executable_path=\"chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "y = 2008\n",
    "\n",
    "driver.get(f'https://www.sports-reference.com/cbb/seasons/men/{y}-ratings.html')\n",
    "driver.maximize_window()\n",
    "element = WebDriverWait(driver, 1000).until(EC.presence_of_element_located((By.XPATH, \"//table\")))\n",
    "with io.StringIO(driver.page_source) as f:\n",
    "    df = pd.read_html(f)[3]\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Sports Raference 2008\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "y = 2008\n",
    "url = f'https://www.sports-reference.com/cbb/seasons/men/{y}-ratings.html'\n",
    "html = requests.get(url).content\n",
    "df_list = pd.read_html(html)\n",
    "df = df_list[-1]  # Assuming the relevant table is the last one\n",
    "df.columns = ['Rk','Team','Conf','','W','L','Pts','Opp','MOV','','SOS','','OSRS','DSRS','SRS','','','']\n",
    "\n",
    "keep = ['Year','Team','W', 'L', 'Pts', 'Opp', 'MOV','SOS', 'OSRS', 'DSRS', 'SRS']\n",
    "\n",
    "df['Year'] = y\n",
    "\n",
    "df = df[keep].dropna()\n",
    "df = df[df['Team']!='School']\n",
    "\n",
    "df.to_csv('step03_br0824.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now scrape 2009 to 2024\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "allbr = pd.read_csv('step03_br0824.csv')\n",
    "\n",
    "allbr = allbr[allbr['Year']<2009]\n",
    "\n",
    "for y in range(2009,2024):\n",
    "    url = f'https://www.sports-reference.com/cbb/seasons/men/{y}-ratings.html'\n",
    "    html = requests.get(url).content\n",
    "    df_list = pd.read_html(html)\n",
    "    df = df_list[-1]  # Assuming the relevant table is the last one\n",
    "    df.columns = ['Rk','Team','Conf','','W','L','Pts','Opp','MOV','','SOS','','OSRS','DSRS','SRS','','','']\n",
    "\n",
    "    keep = ['Year','Team','W', 'L', 'Pts', 'Opp', 'MOV','SOS', 'OSRS', 'DSRS', 'SRS']\n",
    "\n",
    "    df['Year'] = y\n",
    "\n",
    "    df = df[keep].dropna()\n",
    "    df = df[df['Team']!='School']\n",
    "\n",
    "    allbr = pd.concat([allbr,df])\n",
    "\n",
    "allbr.to_csv('step03_br0824.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now grab the current year 2024\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "y = 2024\n",
    "url = f'https://www.sports-reference.com/cbb/seasons/men/{y}-ratings.html'\n",
    "html = requests.get(url).content\n",
    "df_list = pd.read_html(html)\n",
    "df = df_list[-1]  # Assuming the relevant table is the last one\n",
    "df.columns = ['Rk','Team','Conf','','','W','L','Pts','Opp','MOV','','SOS','','OSRS','DSRS','SRS','','','']\n",
    "\n",
    "keep = ['Year','Team','W', 'L', 'Pts', 'Opp', 'MOV','SOS', 'OSRS', 'DSRS', 'SRS']\n",
    "\n",
    "df['Year'] = y\n",
    "\n",
    "df = df[keep].dropna()\n",
    "df = df[df['Team']!='School']\n",
    "\n",
    "allbr = pd.concat([allbr,df])\n",
    "\n",
    "allbr.to_csv('step03_br0824.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75a0be042a4beffa7bfe652d9ca9055e675071b6b5ebd40357e1d67e4cb4d822"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
